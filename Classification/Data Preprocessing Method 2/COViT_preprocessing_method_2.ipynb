{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torchvision.models import VisionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the testing & training data from data_npy folder\n",
    "X_test = [np.load('data_npy/testing/{}'.format(file))\n",
    "          for file in os.listdir('data_npy/testing')]\n",
    "X_train = [np.load('data_npy/training/{}'.format(file))\n",
    "           for file in os.listdir('data_npy/training')]\n",
    "\n",
    "## load the labels and put 1 if the label contains (onlay), 0 if doesn't contain (onlay)\n",
    "y_train = [1 if 'onlay' in file else 0 for file in os.listdir(\n",
    "    'data_npy/training')]\n",
    "y_test = [1 if 'onlay' in file else 0 for file in os.listdir(\n",
    "    'data_npy/testing')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to 4D array\n",
    "X_train = [x.reshape(-1, 128, 128, 128) for x in X_train]\n",
    "\n",
    "X_test = [x.reshape(-1, 128, 128, 128) for x in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load to tensors\n",
    "X_train = torch.FloatTensor(np.array(X_train))\n",
    "X_test = torch.FloatTensor(np.array(X_test))\n",
    "y_train = torch.LongTensor(np.array(y_train))\n",
    "y_test = torch.LongTensor(np.array(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class ViT3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.frozen_block = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "            \n",
    "            \n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.vit = VisionTransformer(\n",
    "            image_size=128,\n",
    "            patch_size=32,\n",
    "            num_classes=2,\n",
    "            num_heads=10,\n",
    "            mlp_dim=2048,\n",
    "            dropout=0.4,\n",
    "            attention_dropout=0.4,\n",
    "            num_layers=10,\n",
    "            hidden_dim=100,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_block(x)\n",
    "        x = x.view(-1, 1, 128, 128).repeat(1, 3, 1, 1)\n",
    "        x = self.vit(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ViT3D().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = [(torchX, torchY)\n",
    "                    for torchX, torchY in zip(X_train, y_train)]\n",
    "test_dataloader = [(torchX, torchY) for torchX, torchY in zip(X_test, y_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "epochs = 500\n",
    "learning_rate = 1e-5\n",
    "momentum = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataloader, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataloader, batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"cse-499a-project-3d-tooth-vit\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"CNN + ViT 3D\",\n",
    "        \"dataset\": \"3D Tooth\",\n",
    "        \"epochs\": epochs,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        # wandb.log({\"loss\": loss})\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test function\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    accuracy = (100*correct)\n",
    "    loss = test_loss\n",
    "    # log metrics to wandb\n",
    "    #wandb.log({\"accuracy\": accuracy, \"avg_loss\": loss})\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "min_loss = 100.0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimiser)\n",
    "    l, a = test(test_dataloader, model, loss_fn)\n",
    "    if(l < min_loss):\n",
    "        min_loss = l\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), \"./saved_models/cnn_with_vit_1_accuracy\"+str(a)+\"__loss\"+str(l)+\".pth\")\n",
    "        print(\"Saved PyTorch Model State to model.pt\")\n",
    "    \n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_no_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
